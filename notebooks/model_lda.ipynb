{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from affinity.load_data.scraper import scrape, scraper\n",
    "import requests\n",
    "import xmltodict\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.plot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data: scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sitemap urls extracted from Conde Nast inventory\n",
    "\n",
    "sitemap_urls = [\n",
    "    \"https://www.houseandgarden.co.uk/sitemap.xml?year=2024&month=6&week=1\",\n",
    "    \"https://www.vanityfair.com/sitemap.xml?year=2024&month=6&week=1\",\n",
    "    \"https://www.epicurious.com/sitemap.xml?year=2024&month=6&week=1\",\n",
    "    \"https://www.epicurious.com/sitemap.xml?year=2024&month=5&week=3\",\n",
    "    \"https://www.cntraveller.com/sitemap.xml?year=2024&month=5&week=5\",\n",
    "    \"https://www.voguebusiness.com/sitemap.xml?year=2024&month=5&week=5\",\n",
    "    \"https://www.voguebusiness.com/sitemap.xml?year=2024&month=5&week=3\",\n",
    "    \"https://pitchfork.com/sitemap.xml?year=2024&month=5&week=4\",\n",
    "    \"https://www.self.com/sitemap.xml?year=2024&month=6&week=1\",\n",
    "    \"https://www.self.com/sitemap.xml?year=2024&month=4&week=3\",\n",
    "    \"https://www.worldofinteriors.com/sitemap.xml?year=2024&month=2&week=3\",\n",
    "    \"https://www.worldofinteriors.com/sitemap.xml?year=2024&month=5&week=4\",\n",
    "    \"https://www.worldofinteriors.com/sitemap.xml?year=2023&month=6&week=2\",\n",
    "    \"https://www.architecturaldigest.com/sitemap.xml?year=2024&month=6&week=1\",\n",
    "    \"https://www.newyorker.com/sitemap.xml?year=2024&month=6&week=1\",\n",
    "    \"https://www.tatler.com/sitemap.xml?year=2024&month=6&week=1\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of base sitemap URLs with placeholders for year, month, and week\n",
    "base_sitemap_urls = [\n",
    "    \"https://www.houseandgarden.co.uk/sitemap.xml?year={year}&month={month}&week={week}\",\n",
    "    \"https://www.vanityfair.com/sitemap.xml?year={year}&month={month}&week={week}\",\n",
    "    \"https://www.epicurious.com/sitemap.xml?year={year}&month={month}&week={week}\",\n",
    "    \"https://www.epicurious.com/sitemap.xml?year={year}&month={month}&week={week}\",\n",
    "    \"https://www.cntraveller.com/sitemap.xml?year={year}&month={month}&week={week}\",\n",
    "    \"https://www.voguebusiness.com/sitemap.xml?year={year}&month={month}&week={week}\",\n",
    "    \"https://www.voguebusiness.com/sitemap.xml?year={year}&month={month}&week={week}\",\n",
    "    \"https://pitchfork.com/sitemap.xml?year={year}&month={month}&week={week}\",\n",
    "    \"https://www.self.com/sitemap.xml?year={year}&month={month}&week={week}\",\n",
    "    \"https://www.self.com/sitemap.xml?year={year}&month={month}&week={week}\",\n",
    "    \"https://www.worldofinteriors.com/sitemap.xml?year={year}&month={month}&week={week}\",\n",
    "    \"https://www.worldofinteriors.com/sitemap.xml?year={year}&month={month}&week={week}\",\n",
    "    \"https://www.worldofinteriors.com/sitemap.xml?year={year}&month={month}&week={week}\",\n",
    "    \"https://www.architecturaldigest.com/sitemap.xml?year={year}&month={month}&week={week}\",\n",
    "    \"https://www.newyorker.com/sitemap.xml?year={year}&month={month}&week={week}\",\n",
    "    \"https://www.tatler.com/sitemap.xml?year={year}&month={month}&week={week}\"\n",
    "]\n",
    "\n",
    "# Generate combinations of URLs by changing year, month, and week\n",
    "sitemap_urls = []\n",
    "for year in range(2020, 2025):\n",
    "    for month in range(1, 13):\n",
    "        for week in range(1, 5):\n",
    "            for base_url in base_sitemap_urls:\n",
    "                sitemap_urls.append(base_url.format(year=year, month=month, week=week))\n",
    "\n",
    "# Print the generated sitemap URLs\n",
    "for url in sitemap_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sitemap_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function for fetching urls for each sitemap\n",
    "\n",
    "def fetch_sitemap_urls(sitemap_url):\n",
    "    try:\n",
    "        response = requests.get(sitemap_url)\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            dict_data = xmltodict.parse(response.content)\n",
    "\n",
    "            urls = [entry['loc'] for entry in dict_data['urlset']['url']]\n",
    "\n",
    "            return urls\n",
    "    except:\n",
    "        return []\n",
    "    else:\n",
    "        print(f\"Failed to fetch {sitemap_url}: Status code {response.status_code}\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiprocess to get sitemap urls for the entire sitemap list\n",
    "\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_sitemap(sitemap_url):\n",
    "    return fetch_sitemap_urls(sitemap_url)\n",
    "\n",
    "all_urls = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Use tqdm to show progress bar\n",
    "    results = list(tqdm(executor.map(process_sitemap, sitemap_urls), total=len(sitemap_urls)))\n",
    "\n",
    "for result in results:\n",
    "    all_urls.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save list of urls as csv\n",
    "\n",
    "inventory_urls= pd.DataFrame()\n",
    "inventory_urls[\"urls\"]=all_urls\n",
    "inventory_urls.to_csv(\"/Users/martafillolbruguera/code/affinity_at_scale/data/inventory_urls.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import inventory_urls to scrape\n",
    "\n",
    "scraped_inventory = pd.read_csv(\"/Users/martafillolbruguera/code/affinity_at_scale/data/inventory_urls.csv\").sample(frac=0.1)\n",
    "\n",
    "scraped_inventory[\"texts\"] = scraped_inventory[\"urls\"].apply(scraper)\n",
    "scraped_inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts = scraped_inventory.loc[scraped_inventory.texts != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean, tokenize, lemmatize\n",
    "\n",
    "def clean (text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, ' ') # Remove Punctuation\n",
    "    lowercased = text.lower() # Lower Case\n",
    "    tokenized = word_tokenize(lowercased) # Tokenize\n",
    "    words_only = [word for word in tokenized if word.isalpha()] # Remove numbers\n",
    "    stop_words = set(stopwords.words('english')) # Make stopword list\n",
    "    without_stopwords = [word for word in words_only if not word in stop_words] # Remove Stop Words\n",
    "    lemma=WordNetLemmatizer() # Initiate Lemmatizer\n",
    "    lemmatized = [lemma.lemmatize(word) for word in without_stopwords] # Lemmatize\n",
    "    cleaned = ' '.join(lemmatized) # Join back to a string\n",
    "    return cleaned\n",
    "\n",
    "# Apply to all texts\n",
    "clean_texts['clean_text'] = clean_texts.texts.apply(clean)\n",
    "\n",
    "clean_texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "data_vectorized = vectorizer.fit_transform(clean_texts['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans to find the k number of clusters to use for the LDA\n",
    "import matplotlib.plot as plt\n",
    "\n",
    "\n",
    "inertias = []\n",
    "ks = range(1,10)\n",
    "for k in ks:\n",
    "    km_test = KMeans(n_clusters=k).fit(#DATA)\n",
    "    inertias.append(km_test.inertia_)\n",
    "\n",
    "plt.plot(ks, inertias)\n",
    "plt.xlabel('k cluster number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply LDA and fit to vectorized data using K means to determine the n of components\n",
    "lda_model = LatentDirichletAllocation(n_components=######)\n",
    "lda_vectors = lda_model.fit_transform(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print topics\n",
    "\n",
    "def print_topics(model, vectorizer):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names_out()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-10 - 1:-1]])\n",
    "print_topics(lda_model, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the model on new data to test it\n",
    "\n",
    "new_data = [\"Tiaras are the fashion accessory for high society brides,but how to wear them correctly? While elegant brides across the nation plan their upcoming nuptials, Tatler answers your questions on exactly how best to sport the regal accessory.\"]\n",
    "\n",
    "new_data_vectorized = vectorizer.transform(new_data) #vectorize first\n",
    "lda_vectors = lda_model.transform(new_data_vectorized) #transform using lda model fitted with our inventory_urls\n",
    "\n",
    "print(\"topic 0 :\", lda_vectors[0][0])\n",
    "print(\"topic 1 :\", lda_vectors[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning with llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract labels from sitemap topics and use them as targets for supervised learning\n",
    "#this will allow us to have a prediction probability not only the closest topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Sentence transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pre trained model\n",
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "targets = [\n",
    "\"Fashion\",\n",
    "\"Pets\",\n",
    "\"Cooking\",\n",
    "\"Fitness\",\n",
    "\"Movies\",\n",
    "\"Gaming\",\n",
    "\"Travel\",\n",
    "\"Cars and automobiles\",\n",
    "\"Outdoor activities\",\n",
    "\"Books\",\n",
    "\"Finance and investments\",\n",
    "\"Business and entrepreneurship\",\n",
    "\"Photography\",\n",
    "\"Art\",\n",
    "\"Social causes and activism\",\n",
    "\"Health and wellness \",\n",
    "\"Gardening\",\n",
    "\"Technology\",\n",
    "\"Education and learning\",\n",
    "\"Sports\",\n",
    "\"Nature\",\n",
    "\"History\",\n",
    "\"Parenting and family\",\n",
    "\"Music\",\n",
    "\"Food and dining \",\n",
    "\"DIY and crafts \",\n",
    "\"Beauty\",\n",
    "\"Science\",\n",
    "\"Politics\"\n",
    "]\n",
    "import pandas as pd\n",
    "interests = pd.DataFrame({\"interest\":targets})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "affinity_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
